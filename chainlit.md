### Set-up Instructions
Enter your OpenAI API key into Replit Secrets as `OPENAI_API_KEY`. Press "Run" button. Close Webview and open in new tab if desired.
The first time you run it, you may encounter this error:
`ImportError: cannot import name dataclass_transform`
This is because Replit in its set-up installs an earlier version of the package `typing-extensions`, while `pydantic` requires a later version. If you encounter an error with typing extensions, input into the shell: 
`pip install pydantic`
And try running it again. 

### API Pricing
Be aware that both context and question have embeddings generated by the OpenAI model text-embedding-ada-002 (priced at $0.0001/1K tokens). The question-answering uses gpt-3.5-turbo (priced at $0.0015/1K input tokens and $0.002/1K output tokens). Tokens, as a rule of thumb, are about 3/4 the number of words.

### Customizing
Currently four parameters exist in `app.py` for customizing the prompt (`PROMPT`) and context included (`CONTEXT_CHUNKS`, `CONTEXT_CHARACTER_LIMIT`), as well as the display of semantic search results (`DISPLAY_LONG_RESULTS`).

**UPDATES**
Model settings may now be changed in `app.py`.

`INCLUDE_SYSTEM_REMINDER` defaults to `False`. if `True`, adds an additional system message to the end of `messages` before submitting the call to the API. I find this can be effective in strongly encouraging certain behaviour.
`PRINT_DEBUG` prints the short version and version with context of the conversation to the console. 

### How to Use
A message with actions shows the current URLs or local text files loaded into the database. You can add additional URLs of YouTube videos, pdf files, or webpages. The system will download that content, split it into chunks, and generate embeddings. You can use the buttons at the top of the chat at any time to add more to the database, and once completed an updated list of sources is shown.

### Why I Made This
I don't think that the differences between generative AI-enabled applications are widely understood. What people don't realize is that many behind-the-scenes decisions influence the nature of the output. Many are now trying to build applications with generative AI which aim to provide correct answers to questions based on source material. However, this is much more difficult than many realize. I hope more people will understand and ask questions about generative AI applications.

### How it Works
#### Open-source Python Packages
It uses [embedchain](https://github.com/embedchain/embedchain/tree/main), a wrapper around [LangChain](https://github.com/hwchase17/langchain) and [chromadb](https://github.com/chroma-core/chroma). Embedchain handles loading data by URL or entering text, splitting it into chunks, generating [embeddings](https://platform.openai.com/docs/guides/embeddings), and saving these to a local vector database. When a user query is submitted, the system retrieves chunks of context information, formats a prompt, submits that prompt to gpt-3.5-turbo and returns the answer.

#### Prompt and context
The default prompt and handling of context of EmbedChain have been over-ridden and turned into settings parameters found at the top of `app.py`.

The system also has the option of retrieving additional results and displaying them in a Markdown table in a text element below a message, controlled by `DISPLAY_LONG_RESULTS` (`True`/`False`).

This version of the app should not be used by multiple users unless they are trusted, as any user can add or delete any embeddings and sees the same sources.

#### Text splitting
The main component that is not customizeable is the size of the context chunks as set in EmbedChain. The character sizes are: 
* 300 for local text files
* 500 for web page
* 1000 for PDF files
* 2000 for Youtube videos

They're split using [Langchain's recommended TextSplitter](https://js.langchain.com/docs/modules/indexes/text_splitters/examples/recursive_character) `RecursiveCharacterTextSplitter` from `langchain.text_splitter`.





